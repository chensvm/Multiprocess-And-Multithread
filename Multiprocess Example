# Dependency Tree
import os
import pickle, time
from itertools import chain
import multiprocessing
from multiprocessing import Pool
from nltk.parse.stanford import StanfordDependencyParser
os.environ['STANFORD_PARSER'] = '/Users/chensvm/stanford_nltk/stanford-parser-full-2018-10-17/stanford-parser.jar'
os.environ['STANFORD_MODELS'] = '/Users/chensvm/stanford_nltk/stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar'
model_name = "/Users/chensvm/stanford_nltk/stanford-english-corenlp-2018-10-05-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"
dep_parser=StanfordDependencyParser(model_name)

def lca(tree, index1, index2):
    node = index1
    path1 = []
    path2 = []
    path1.append(index1)
    path2.append(index2)
    while(node != tree.root):
        node = tree.nodes[node['head']]
        path1.append(node)
    node = index2
    while(node != tree.root):
        node = tree.nodes[node['head']]
        path2.append(node)
    for l1, l2 in zip(path1[::-1],path2[::-1]):
        if(l1==l2):
            temp = l1
    return temp

def path_lca(tree, node, lca_node):
    path = []
    path.append(node)
    while(node != lca_node):
        node = tree.nodes[node['head']]
        path.append(node)
    return path

def seq(tree, lca):
    l=[lca]
    for key in tree.nodes[lca]['deps']:
        for i in tree.nodes[lca]['deps'][key]:
            l.extend(seq(i))
    return l




def make_feature(sentences, e1, e2):
    words_seq = []
    pos_tags_seq = []
    deps_seq = []
    word_path1 = []
    word_path2 = []
    dep_path1 = []
    dep_path2 = []
    pos_tags_path1 = []
    pos_tags_path2 = []
    childs_path1 = []
    childs_path2 = []
    pos_path1 = []
    pos_path2 = []

    for i in range(len(sentences)):
        try:

            parse_tree = dep_parser.parse(sentences[i])
            for trees in parse_tree:
                tree = trees

            word2pos = dict((tree.nodes[k]['address'], j) for k, j in zip(tree.nodes, range(len(tree.nodes))))
            pos2word = dict((j, tree.nodes[k]['address']) for k, j in zip(tree.nodes, range(len(tree.nodes))))

            pos_tags_seq.append([tree.nodes[k]['tag'] for k in tree.nodes][1:])
            words_seq.append([tree.nodes[k]['word'] for k in tree.nodes][1:])
            deps_seq.append([tree.nodes[k]['rel'] for k in tree.nodes][1:])

            node1 = tree.nodes[e1[i] + 1]
            node2 = tree.nodes[e2[i] + 1]
            if node1['address'] != None and node2['address'] != None:
                print(i, "success")
                lca_node = lca(tree, node1, node2)
                path1 = path_lca(tree, node1, lca_node)
                path2 = path_lca(tree, node2, lca_node)[:-1]

                word_path1.append([p["word"] for p in path1])
                word_path2.append([p["word"] for p in path2])
                dep_path1.append([p["rel"] for p in path1])
                dep_path2.append([p["rel"] for p in path2])
                pos_tags_path1.append([p["tag"] for p in path1])
                pos_tags_path2.append([p["tag"] for p in path2])

                pos_path1.append([word2pos[node['address']] for node in path1])
                pos_path2.append([word2pos[node['address']] for node in path2])
                childs = [sorted(chain.from_iterable(node['deps'].values())) for node in path1]
                childs_path1.append([[word2pos[c] for c in child] for child in childs])
                childs = [sorted(chain.from_iterable(node['deps'].values())) for node in path2]
                childs_path2.append([[word2pos[c] for c in child] for child in childs])

            else:
                print(i, node1["address"], node2["address"])
        except AssertionError:
            print(i, "error")
    return words_seq,pos_tags_seq,deps_seq,word_path1,word_path2,dep_path1,dep_path2,pos_tags_path1,pos_tags_path2,childs_path1,childs_path2,pos_path1,pos_path2

if __name__ == '__main__':
    process_num = 10
    f = open('../my_data/train_data', 'rb')
    sentences, e1, e2, flag = pickle.load(f)
    f.close()

    start_time = time.time()
    pool = Pool(processes=process_num)
    print(len(sentences))
    words_seq = []
    pos_tags_seq = []
    deps_seq = []
    word_path1 = []
    word_path2 = []
    dep_path1 = []
    dep_path2 = []
    pos_tags_path1 = []
    pos_tags_path2 = []
    childs_path1 = []
    childs_path2 = []
    pos_path1 = []
    pos_path2 = []
    result = []
    length = len(sentences)-130
    sentences = sentences[:length]

    part_len = int(length/process_num)
    for i in range(process_num):
        if i != process_num-1:
            sentences_temp = sentences[i*part_len:(i+1)*part_len]
            e1_temp = e1[i*part_len:(i+1)*part_len]
            e2_temp = e2[i*part_len:(i+1)*part_len]
        else:
            sentences_temp = sentences[i * part_len:]
            e1_temp = e1[i * part_len:]
            e2_temp = e2[i * part_len:]

        result.append(pool.apply_async(make_feature, args=(sentences_temp, e1_temp, e2_temp)))
    pool.close()
    pool.join()
    for process_feature in result:
        temp = process_feature.get()
        words_seq.extend(temp[0])
        pos_tags_seq.append(temp[1])
        deps_seq.append(temp[2])
        word_path1.append(temp[3])
        word_path2.append(temp[4])
        dep_path1.append(temp[5])
        dep_path2.append(temp[6])
        pos_tags_path1.append(temp[7])
        pos_tags_path2.append(temp[8])
        childs_path1.append(temp[9])
        childs_path2.append(temp[10])
        pos_path1.append(temp[11])
        pos_path2.append(temp[12])

    end_time = time.time()
    print(end_time-start_time)
